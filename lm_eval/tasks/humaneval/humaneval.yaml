task: humaneval
dataset_path: openai/openai_humaneval
output_type: generate_until
test_split: test
doc_to_text: "{{prompt}}"
doc_to_target: !function utils.build_references
metric_list:
  - metric: !function utils.pass_at_1
    aggregation: mean
    higher_is_better: true
# set according to bigcode harness defaults here https://github.com/bigcode-project/bigcode-evaluation-harness/blob/334efb7feb1c6d18fc4be2c71ae73248f3de8440/bigcode_eval/arguments.py#L16
generation_kwargs:
  until:
    - "\nclass"
    - "\ndef"
    - "\n#"
    - "\nif"
    - "\nprint"
  do_sample: true
  temperature: 0.2
  top_p: 0.95
repeats: 1
num_fewshot: 0
filter_list:
  - name: "n=1" # number of samples to estimate pass@k
    filter:
      - function: "custom"
        filter_fn: !function utils.build_predictions
metadata:
  version: 1.0